# Modern LLM Pipeline Report

**Run Name:** tacc-full
**Generated:** 2025-11-27 01:32:18

## Model Architecture

| Parameter | Value |
|-----------|-------|
| Hidden Dimension | 1024 |
| Layers | 12 |
| Attention Heads | 16 |
| FFN Hidden Size | 4096 |
| Max Sequence Length | 1024 |
| Vocabulary Size | 50257 |
| RoPE | Yes |
| Attention Sinks | No |
| SwiGLU | Yes |
| GQA | No |

## Pipeline Stages

### 1. Pretraining
- **Checkpoint:** `/work/09999/aymanmahfuz/ls6/modern_llm/checkpoints/tacc-full-pretrain/tacc-full-pretrain_final.pt`
- **Max Steps:** 20000
- **Learning Rate:** 0.0003
- **Batch Size:** 128

### 2. Supervised Fine-Tuning (SFT)
- **Checkpoint:** `/work/09999/aymanmahfuz/ls6/modern_llm/checkpoints/tacc-full-sft/tacc-full-sft_final.pt`
- **Dataset:** tatsu-lab/alpaca
- **Max Steps:** 3000
- **Learning Rate:** 1e-05

### 3. Direct Preference Optimization (DPO)
- **Checkpoint:** `/work/09999/aymanmahfuz/ls6/modern_llm/checkpoints/tacc-full-dpo/tacc-full-dpo_final.pt`
- **Dataset:** Anthropic/hh-rlhf
- **Beta:** 0.1
- **Max Steps:** 2000

### 4. Verifier
- **Checkpoint:** `/work/09999/aymanmahfuz/ls6/modern_llm/checkpoints/tacc-full-verifier/tacc-full-verifier_final.pt`
- **Max Steps:** 2000

## Evaluation Results

### Perplexity Comparison

| Stage | Perplexity | Loss | Parameters |
|-------|------------|------|------------|
| BASE | 2900.55 | 7.9727 | 252.9M |
| SFT | 1745.72 | 7.4649 | 252.9M |
| DPO | 1731.32 | 7.4566 | 252.9M |

## Sample Generations

**Prompt:** Once upon a time

**Output:** Once upon a time with a personal situation. The case of the statement is that the statement is wrong and that it is not determined that it is an appropriate part of the situation that people are in the world. This is a result of the conflict that is not there who

**Prompt:** The meaning of life is

**Output:** The meaning of life is equivalent to one author or author of the oldest Mkhedruli letters . The oldest Mkhedruli inscription is found in Ateni Sgithean ( the polyn Ba of Antig ) of Nuskhuri , who is

**Prompt:** In machine learning,

**Output:** In machine learning, and real combat are addressed to rescue charges throughout the Vietnam War in order to achieve independence on the Pacific Ocean. This situation has also been prompted by the ranks of the US government of the Republic of Vietnam and the 1960s who tried to fight the war

## Summary

This report summarizes the Modern LLM pipeline execution.
The pipeline implements a frontier-style training workflow:

1. **Pretraining** - Language model training on text corpora
2. **SFT** - Instruction tuning (Ouyang et al., 2022)
3. **DPO** - Preference alignment (Rafailov et al., 2023)
4. **Verifier** - Answer correctness scoring (Lightman et al., 2023)

---

*Report generated by Modern LLM Pipeline*