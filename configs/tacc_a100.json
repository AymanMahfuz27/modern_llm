{
  "run_name": "modern-llm-tacc-a100",
  "vocab_size": 50257,
  "d_model": 1024,
  "n_layers": 16,
  "n_heads": 16,
  "ffn_hidden_size": 4096,
  "max_seq_len": 2048,
  "dropout": 0.1,
  "use_rope": true,
  "use_attention_sinks": true,
  "num_attention_sinks": 4,
  "use_swiglu": true,
  "tie_embeddings": true,
  "use_gqa": false,
  "use_moe": false,

  "hardware_preset": "a100",
  "data_preset": "large",

  "pretrain_max_steps": 100000,
  "pretrain_lr": 3e-4,
  "pretrain_batch_size": 256,
  "pretrain_micro_batch_size": 16,
  "pretrain_warmup_steps": 2000,

  "sft_max_steps": 10000,
  "sft_lr": 1e-5,
  "sft_batch_size": 64,
  "sft_micro_batch_size": 8,
  "sft_dataset": "tatsu-lab/alpaca",

  "dpo_max_steps": 5000,
  "dpo_lr": 5e-6,
  "dpo_batch_size": 32,
  "dpo_micro_batch_size": 4,
  "dpo_beta": 0.1,
  "dpo_dataset": "Anthropic/hh-rlhf",

  "verifier_max_steps": 5000,
  "verifier_lr": 1e-4,
  "verifier_batch_size": 64,
  "verifier_micro_batch_size": 8,

  "output_dir": "experiments/runs",
  "tokenizer_name": "gpt2",
  "seed": 42,
  "mixed_precision": "bf16",
  "eval_every": 1000,
  "save_every": 5000,
  "log_every": 100
}


