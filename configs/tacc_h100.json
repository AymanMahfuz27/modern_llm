{
  "run_name": "modern-llm-tacc-h100",
  "vocab_size": 50257,
  "d_model": 1024,
  "n_layers": 24,
  "n_heads": 16,
  "ffn_hidden_size": 4096,
  "max_seq_len": 2048,
  "dropout": 0.1,
  "use_rope": true,
  "use_attention_sinks": true,
  "num_attention_sinks": 4,
  "use_swiglu": true,
  "tie_embeddings": true,
  "use_gqa": true,
  "gqa_groups": 4,
  "use_moe": false,

  "hardware_preset": "h100",
  "data_preset": "xl",

  "pretrain_max_steps": 200000,
  "pretrain_lr": 3e-4,
  "pretrain_batch_size": 512,
  "pretrain_micro_batch_size": 32,
  "pretrain_warmup_steps": 4000,

  "sft_max_steps": 15000,
  "sft_lr": 1e-5,
  "sft_batch_size": 128,
  "sft_micro_batch_size": 16,
  "sft_dataset": "tatsu-lab/alpaca",

  "dpo_max_steps": 8000,
  "dpo_lr": 5e-6,
  "dpo_batch_size": 64,
  "dpo_micro_batch_size": 8,
  "dpo_beta": 0.1,
  "dpo_dataset": "Anthropic/hh-rlhf",

  "verifier_max_steps": 8000,
  "verifier_lr": 1e-4,
  "verifier_batch_size": 128,
  "verifier_micro_batch_size": 16,

  "output_dir": "experiments/runs",
  "tokenizer_name": "gpt2",
  "seed": 42,
  "mixed_precision": "bf16",
  "eval_every": 2000,
  "save_every": 10000,
  "log_every": 100
}



